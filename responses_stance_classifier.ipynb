{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Introduction**\n",
    "\n",
    "In this notebook we build a small stance classification demo using LangChain and a Hugging Face text-generation model.\n",
    "Given a comment and its reply, the model should output one word: agree, disagree, or neutral.\n",
    "\n",
    "We will:\n",
    "\n",
    "\n",
    "\n",
    "*   Install the needed libraries.\n",
    "*   Create a prompt that explains the task.\n",
    "*   Load a Hugging Face model and wrap it with LangChain.\n",
    "*   Run a simple chain on a few examples.\n",
    "*   Improve results a bit using a few-shot prompt (showing examples in the prompt)."
   ],
   "metadata": {
    "id": "hpW5sSpbHls3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Installing libraries**\n",
    "Installing LangChain, Transformers, and the LangChain–Hugging Face bridge.\n",
    "\n",
    "Notes:\n",
    "\n",
    "If using a GPU, we would also need PyTorch with CUDA for your environment.\n",
    "\n",
    "If we hit “No backend found”, we would need to install PyTorch (pip install torch --index-url https://download.pytorch.org/whl/cu121 for CUDA 12.1, or the CPU wheel).\n",
    "\n",
    "We can safely re-run installs; pip will skip existing packages.\n"
   ],
   "metadata": {
    "id": "W7dRvKfeJKGh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WhziW3ByboA",
    "outputId": "c7a5c9a3-b80e-49a6-f7d3-6b15324e39a9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.28)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.12/dist-packages (0.3.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.3.76)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.35.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.10)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.28)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.11.9)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2025.8.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain\n",
    "! pip install transformers\n",
    "! pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Building a reusable prompt**\n",
    "\n",
    "Creating a PromptTemplate that takes two inputs—comment and reply—and asks the model to output only one label: agree, disagree, or neutral.\n",
    "\n",
    "\n",
    "this will Keep the prompt in one place and makes it easy to reuse and reduces mistakes.\n",
    "\n",
    "We have to be strict in prompts when we want short, clean outputs. We clearly say “output no other words”."
   ],
   "metadata": {
    "id": "_aeJQqf7MxZq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BGTEiFtUylVo",
    "outputId": "1d616c05-7d15-4d3c-f7b5-702a31050494"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Please classify the stance, or opinion, of the following reply to the comment. Note that we want the stance of the reply to the comment, and not the stance of the reply to topic of the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words, only the label.\n",
      "comment: Using phones right before bed makes it harder to fall asleep.\n",
      "reply: I don’t think so; I fall asleep right after scrolling.\n",
      "stance:\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define the template for stance classification\n",
    "template = '''Please classify the stance, or opinion, of the following reply to the comment. Note that we want the stance of the reply to the comment, and not the stance of the reply to topic of the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words, only the label.\n",
    "comment: {comment}\n",
    "reply: {reply}\n",
    "stance:'''\n",
    "\n",
    "# Create the prompt object\n",
    "stance_prompt = PromptTemplate(\n",
    "    input_variables=[\"comment\", \"reply\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Example usage of the prompt\n",
    "comment = \"Using phones right before bed makes it harder to fall asleep.\"\n",
    "reply = \"I don’t think so; I fall asleep right after scrolling.\"\n",
    "prompt = stance_prompt.format(comment=comment, reply=reply)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Loading a Hugging Face model and wrap it for LangChain**\n",
    "\n",
    "Creating a transformers.pipeline for text2text-generation.\n",
    "\n",
    "and then wrapping it with HuggingFacePipeline so LangChain can call it like an LLM.\n",
    "\n",
    "Send our stance prompt to the model and prints the raw response.\n",
    "\n",
    "-> Make sure the model name exists on Hugging Face and we have access. If it’s private, we need to pass a token.\n",
    "\n",
    "->If we don’t have a GPU, we need to set device=-1.\n",
    "\n",
    "->If we see a torch/TensorFlow/Flax error, we need to install torch at minimum."
   ],
   "metadata": {
    "id": "Gaiy6FpNTvqU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# we use smaller model that fits on Colab GPUs\n",
    "MODEL_ID = \"google/flan-t5-large\"   # can try \"google/flan-t5-large\" if VRAM allows\n",
    "\n",
    "hf_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=MODEL_ID,\n",
    "    device=0 if torch.cuda.is_available() else -1,  # GPU if available, else CPU\n",
    "    max_new_tokens=50,     # only need 1 word\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "# Example usage with the prompt object from before\n",
    "prompt = '''Please classify the stance, or opinion, of the following reply to the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words.\n",
    "comment: Using phones right before bed makes it harder to fall asleep.\n",
    "reply: I agree,once I stopped scrolling at night, I slept faster.\n",
    "stance:'''\n",
    "\n",
    "# Get the model's response\n",
    "response = llm(prompt)\n",
    "print(response)\n"
   ],
   "metadata": {
    "id": "weRalTO3wpcp",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277,
     "referenced_widgets": [
      "803d50d131fa408ea283295d37a40d7b",
      "6feaa5062d034b6cada721a3ff52c37e",
      "a891f42223524170a96467ca30aca621",
      "df37fcf41aa74e3fa21e31bbde502066",
      "9022417d463f4886a93e5006a9a9fb56",
      "6ea7204377384f4292097905d7552e27",
      "815b61b4640444feb2ce9836d506d40c",
      "f29fc788bd6f4ac096d95c7fba08af31",
      "0243d6473ff447a1bb25d391973891cd",
      "34e7938ab9fb47baa404290397ecd022",
      "4717e2c085c040c9b838ab35f8d14a75",
      "99190be042a24b8e93ba7e1e3f791141",
      "be7f2f287f9f4f7ebe24fe4d455357ba",
      "bd48eaf71e34454ab0a51f9d3043a47a",
      "40e82f2df4574079a8ed4306e1150cd0",
      "047ff24f89cb4ceea039f5070707757c",
      "a37b80476c154c31af99d66e370c4799",
      "4376299899024067bdc0cfeaa2ada49c",
      "73be4129ce964c9082128675b566328c",
      "ee05c29e5bde4886a67992f550b61aa7",
      "498eafe226c44be3a985cc05fdf09d1a",
      "341c867755e74db480f3948cc7fc3c54",
      "e59f9976ed764c8db72988107d287b92",
      "ba12901a692c44e2a315aa4f6d3605bb",
      "75a230df05a34e96af597e44f1cfb7d6",
      "39fad0f751ab40bd80821e964e1aad77",
      "1a20dca7a854403c90991d90d15f15d4",
      "485ee4a5a9f84b4c9a175e5f19b335f9",
      "6f3395d803414e568c9848b8e664e12b",
      "cfbe164795a54b86b13e034df25283d6",
      "0d4df8d32017482ea83b633c0847183b",
      "0003677f39f7428c91e8326d09ec9456",
      "18b1d55cb7e844dc93d0580ec67b58f3",
      "a52d09d707a6404baf720b197f2daa04",
      "db5cefb36f9c4a99a9fc90e6e65d2aad",
      "f9d72cc56b594318a15df71aa18deb5d",
      "41ced2ba928943d394e73769e9796f9f",
      "4b0351b0f7d34716b61cdcd58dfbed62",
      "b0fc7aff683c45dd926f0f915c8ba920",
      "274ff8f56c8e4c5b8e8ae007742e9b52",
      "b65a87d24e1b40d68f0b953491db37ff",
      "316d440c02544680bb32fb2b0bd34ef7",
      "54b0637509584068bf6c7713932f4499",
      "232fa1fadef742c7a46980e69b838d72",
      "f0324c8afaab41b08ce0bd26a81b22fd",
      "e8eff6dfb5114d38b26ee6fcca5ff75e",
      "956ad60b8c8340538ffe92c6cf97e85c",
      "b43d67b40be34cd4bda16144f1a7d8fd",
      "6718443c3d7c4a5ab903ac6fe47c1b13",
      "c36912053fed42f0ac2d57118583b752",
      "1bf97d728f59448094a17c8fe5774bc6",
      "cda47e84027f42d5a9bc1e13e8e2e422",
      "5ebf972c2f5f4f61a528a380af3c3c5d",
      "665f35a42c2a44b7afb4f05f84bddc82",
      "56a9ff2a9dab4331a18bd93ddf739e8b",
      "0feea0305206435faa6e0e1c01eba209",
      "fbc78c51f6864131a3892a16486f1bf6",
      "4341272fee804d50b1c8d976910e0489",
      "ba4fe4b63f4041a49b24b569faf0c0cb",
      "961cfd073168413f86c90bae2793d027",
      "319f8343ce39485c986b4d50238db57e",
      "3454221045ff46a9ba7a3d74af820b95",
      "e5f548b3c0a5493c8f66beda9f10b86f",
      "b94fb9895c834a9590a160c2e95ae20e",
      "249128d7fe67431186ca37afbc0ce012",
      "92f09f8c555748af83857aa249eec00f",
      "7ae515bcc55f456a8868e179ade01fea",
      "c46130dcbbe44d6899a303dfb99e4184",
      "f4d5917278a6496cbe365431887fab58",
      "3509de30b623463684541419e4feb03c",
      "29a0b2c711f44f459f8495e41aac66c8",
      "071172ba06664516ae8272ecb56d78d8",
      "3e8a3347d3f6476f8421f6ef72035a92",
      "1f1d3f1e42db4a8a92268e030397fa83",
      "9ffa99bbc6c54528b2d9e86a642af856",
      "a7566e9a00d34269b82ffcae3f1b1b91",
      "a5232ba4602646cab87210671da77b84"
     ]
    },
    "outputId": "2083a735-0ac6-4101-a31f-05d668b77d23"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "803d50d131fa408ea283295d37a40d7b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99190be042a24b8e93ba7e1e3f791141"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e59f9976ed764c8db72988107d287b92"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a52d09d707a6404baf720b197f2daa04"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0324c8afaab41b08ce0bd26a81b22fd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0feea0305206435faa6e0e1c01eba209"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ae515bcc55f456a8868e179ade01fea"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "agree\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Creating a chain: prompt → model**\n",
    "\n",
    "We will Build a simple chain using the pipe operator (|).\n",
    "\n",
    "stance_prompt | llm : formats the prompt with inputs, then send it to the model.\n",
    "\n",
    "We will run .invoke({...}) with a comment and reply and print the result.\n",
    "\n",
    "->Chains are clean and composable. We can later add more steps (e.g., parsing, validation) without changing the rest, that's why it is useful"
   ],
   "metadata": {
    "id": "NfhWjSJJUp_a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5AJl7qzW3Ul9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "717633bd-5411-411b-be74-fbc74b6885c7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "agree\n"
     ]
    }
   ],
   "source": [
    "stance_chain = stance_prompt | llm\n",
    "\n",
    "# Example usage: run the chain with the provided comment and reply\n",
    "comment = \"Taking a 20-minute walk each day improves mood and energy.\"\n",
    "reply = \"I agree, even a short walk makes me feel more alert.\"\n",
    "\n",
    "# Format the input and get the result\n",
    "result = stance_chain.invoke({\"comment\": comment, \"reply\": reply})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "**Preparing the test data**\n",
    "\n",
    "We will define one fixed comment and a small list of replies with carying stances.\n",
    "\n",
    "This will give us a quick way to see how the model behaves with different kinds of replies: on-topic, off-topic, and noisy."
   ],
   "metadata": {
    "id": "TLD6mnkeVmkQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "O2xgNEu24Cvy"
   },
   "outputs": [],
   "source": [
    "test_comment = \"Using phones right before bed makes it harder to fall asleep.\"\n",
    "\n",
    "test_replies = [\n",
    "    \"Totally—my mind keeps racing after browsing.\",\n",
    "    \"I’m fine; I can sleep in minutes even after watching videos.\",\n",
    "    \"Maybe, but only if the room is bright.\",\n",
    "    \"Once I put my phone away an hour before bed, I slept better.\",\n",
    "    \"Nah, dark mode and low brightness solve it.\",\n",
    "    \"Hard to say; caffeine and stress affect me more.\",\n",
    "    \"Lol I scroll every night and still pass out.\",\n",
    "    \"Blue light filters help a bit, but not always.\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Running the chain on all test replies**\n",
    "\n",
    "We will loop through every reply, call the chain, store the outputs, and print a neat summary.\n",
    "\n",
    "-> We must check if the model return only one of agree / disagree / neutral?\n",
    "\n",
    "-> Does the model stay on task when the reply is off-topic?\n"
   ],
   "metadata": {
    "id": "kHyWaVibWePS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dpUvunQA4JKd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1ad99afe-6b4c-4f6e-de05-7b6179dab28e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reply 1: Totally—my mind keeps racing after browsing.\n",
      "Stance: agree\n",
      "\n",
      "Reply 2: I’m fine; I can sleep in minutes even after watching videos.\n",
      "Stance: agree\n",
      "\n",
      "Reply 3: Maybe, but only if the room is bright.\n",
      "Stance: agree\n",
      "\n",
      "Reply 4: Once I put my phone away an hour before bed, I slept better.\n",
      "Stance: agree\n",
      "\n",
      "Reply 5: Nah, dark mode and low brightness solve it.\n",
      "Stance: disagree\n",
      "\n",
      "Reply 6: Hard to say; caffeine and stress affect me more.\n",
      "Stance: neutral\n",
      "\n",
      "Reply 7: Lol I scroll every night and still pass out.\n",
      "Stance: agree\n",
      "\n",
      "Reply 8: Blue light filters help a bit, but not always.\n",
      "Stance: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for reply in test_replies:\n",
    "    response = stance_chain.invoke({\"comment\": test_comment, \"reply\": reply})\n",
    "    responses.append(response)\n",
    "\n",
    "# Print the results\n",
    "for idx, (reply, response) in enumerate(zip(test_replies, responses)):\n",
    "    print(f\"Reply {idx+1}: {reply}\\nStance: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Building a few-shot prompt**\n",
    "\n",
    "Creating a FewShotPromptTemplate with several labeled examples.\n",
    "\n",
    "The prefix will explain the task.\n",
    "\n",
    "The examples will show the desired input/output format.\n",
    "\n",
    "The suffix will hold the final query (our actual comment and reply).\n"
   ],
   "metadata": {
    "id": "sGq8D2oLW75w"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NgTwLv--78ST"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate\n",
    "\n",
    "# Define the prompt template for each example\n",
    "example_template = '''comment: {comment}\n",
    "reply: {reply}\n",
    "stance: {stance}'''\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"comment\", \"reply\", \"stance\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# Define the examples with various stances\n",
    "examples = [\n",
    "    {\n",
    "        'comment': \"Doing a 5-minute stretch each morning improves flexibility.\",\n",
    "        'reply': \"Yes, I feel looser after a week of daily stretches.\",\n",
    "        'stance': 'agree'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"Listening to music while studying improves concentration.\",\n",
    "        'reply': \"Not for me—lyrics pull my attention away.\",\n",
    "        'stance': 'disagree'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"Drinking eight glasses of water a day reduces headaches.\",\n",
    "        'reply': \"It might help some people, but not always for me.\",\n",
    "        'stance': 'neutral'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"Eating breakfast helps you focus better for the rest of the morning.\",\n",
    "        'reply': \"Sometimes, but if I eat heavy I feel sleepy.\",\n",
    "        'stance': 'neutral'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"Keeping houseplants in a room improves indoor air quality.\",\n",
    "        'reply': \"I doubt it; my allergies got worse with plants around.\",\n",
    "        'stance': 'disagree'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"A 20-minute afternoon nap boosts memory.\",\n",
    "        'reply': \"Yes, I recall details better after short naps.\",\n",
    "        'stance': 'agree'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"Using checklists reduces mistakes in daily tasks.\",\n",
    "        'reply': \"Absolutely—my to-do list catches things I forget.\",\n",
    "        'stance': 'agree'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"Playing puzzle games every day sharpens attention.\",\n",
    "        'reply': \"I don’t think so; I saw no change after months of puzzles.\",\n",
    "        'stance': 'disagree'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the prefix and suffix for the few-shot prompt\n",
    "prefix = '''Stance classification is the task of determining the expressed or implied opinion, or stance, of a reply toward a comment. The following replies express opinions about the associated comment. Each reply can either be \"agree\", \"disagree\", or \"neutral\" toward the comment.'''\n",
    "\n",
    "suffix = '''Analyze the following reply to the provided comment and determine its stance. Respond with a single word: \"agree\", \"disagree\", or \"neutral\". Only return the stance as a single word, and no other text.\n",
    "comment: {comment}\n",
    "reply: {reply}\n",
    "stance:'''\n",
    "\n",
    "# Create the FewShotPromptTemplate using the provided prefix, suffix, and examples\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"comment\", \"reply\"],\n",
    "    example_separator=\"\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Run the few-shot chain on the same test set**\n",
    "\n",
    "We will Pipe the few-shot prompt into the same llm.\n",
    "\n",
    "Run the test replies again and print the stances.\n",
    "\n",
    "What we are checking here is :\n",
    "\n",
    "->Are the labels more stable?\n",
    "\n",
    "->Did off-topic replies become more often neutral?\n",
    "\n",
    "->Are there fewer extra words in the output?\n"
   ],
   "metadata": {
    "id": "xmN4EhphXVQ6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "YiUO5EI28BY2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f55e1907-0dcd-40be-d2d7-32fe9c3495d4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['agree']\n",
      "['agree', 'disagree']\n",
      "['agree', 'disagree', 'disagree']\n",
      "['agree', 'disagree', 'disagree', 'agree']\n",
      "['agree', 'disagree', 'disagree', 'agree', 'disagree']\n",
      "['agree', 'disagree', 'disagree', 'agree', 'disagree', 'neutral']\n",
      "['agree', 'disagree', 'disagree', 'agree', 'disagree', 'neutral', 'agree']\n",
      "['agree', 'disagree', 'disagree', 'agree', 'disagree', 'neutral', 'agree', 'agree']\n",
      "Reply 1: Totally—my mind keeps racing after browsing.\n",
      "Stance: agree\n",
      "\n",
      "Reply 2: I’m fine; I can sleep in minutes even after watching videos.\n",
      "Stance: disagree\n",
      "\n",
      "Reply 3: Maybe, but only if the room is bright.\n",
      "Stance: disagree\n",
      "\n",
      "Reply 4: Once I put my phone away an hour before bed, I slept better.\n",
      "Stance: agree\n",
      "\n",
      "Reply 5: Nah, dark mode and low brightness solve it.\n",
      "Stance: disagree\n",
      "\n",
      "Reply 6: Hard to say; caffeine and stress affect me more.\n",
      "Stance: neutral\n",
      "\n",
      "Reply 7: Lol I scroll every night and still pass out.\n",
      "Stance: agree\n",
      "\n",
      "Reply 8: Blue light filters help a bit, but not always.\n",
      "Stance: agree\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_chain = few_shot_prompt | llm\n",
    "\n",
    "responses = []\n",
    "for reply in test_replies:\n",
    "    response = few_shot_chain.invoke({\"comment\": test_comment, \"reply\": reply})\n",
    "    responses.append(response)\n",
    "    print(responses)\n",
    "\n",
    "# Print the results\n",
    "for idx, (reply, response) in enumerate(zip(test_replies, responses)):\n",
    "    print(f\"Reply {idx+1}: {reply}\\nStance: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}